{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Parallel STL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- _Code_: [Hello, PSTL!](#Hello,-PSTL!)\n",
    "- _Code_: [Fancy Iterators](#Fancy-iterators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises in this notebook mainly consists of 3 parts. Description in the first cell. Code example in the second cell. You can modify the code inline and run the cell to create C++ source code. A following solution code cell is optional and is designed to overwrite the code snipped with a solution. Last cell is used to compile and run the exercise on the DevCloud batch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, PSTL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we'll build and run a very simple C++ application that invokes the classic STL algorithm ``sort`` on an input sequence of random integers.\n",
    "\n",
    "To make this exercise more interesting we parallelize our sort at a high-level. To do that, we simply add a parallel execution policy and take the algorithm for a second run. Adding a parallel execution policy is as easy as inserting `std::execution::par` as a first parameter. This tells the compiler and runtime that it's safe to execute iterations in parallel on multiple threads of execution. It's a hint by the user but not a mandatory requirement to run any optimizations for the runtime. A high-quality implementation like Parallel STL with TBB as a backend could benefit by splitting the sort into finer grained tasks. Those can be executed by TBB's scheduler in a multithreaded fashion. Of course that adds some overhead, such as maintaining a thread pool or synchronize the tasks. So it's not advised to add a parallel execution policy to each and every algorithm. It's a user's (Yes that's you) responsibility to ensure a parallel optimization hint is legal and beneficial.\n",
    "\n",
    "Consequently, in our simple example we time both runs so that we can compare them. Please note that intentionally any warm-up for the parallel execution is skipped.\n",
    "\n",
    "Inspect the code below - there are no modifications necessary. Run the first cell to create the file, then run the cell below it to compile and execute the code.\n",
    "1. Inspect the code cell below, then click run ▶ to save the code to a file\n",
    "2. Run ▶ the cell in the __Build and Run__ section below the code snippet to compile and execute the code in the saved file\n",
    "\n",
    "What are your expectations regarding the speedup? We're likely running on an system with two Intel Xeon processor with 6 cores each (12 hardware threads) in the current DevCloud configuration, as of Sept'20. \n",
    "\n",
    "What happens if we decrease the size of the input sequence to let's say 50? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sort.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <random>\n",
    "\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "int main() {\n",
    "  std::default_random_engine e1(0);\n",
    "  std::uniform_int_distribution<int> d(0, 5000);\n",
    "\n",
    "  // initialize input vector with random numbers\n",
    "  std::vector<int> data(5000000);\n",
    "  for(auto& i:data) {\n",
    "    i = d(e1);\n",
    "  }\n",
    "\n",
    "  {\n",
    "    std::vector<int> input(data);\n",
    "    std::cout << \"Running sequentially:\\n\";\n",
    "    auto st0 = std::chrono::high_resolution_clock::now();\n",
    "    std::sort(input.begin(), input.end());\n",
    "    auto st1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Serial time   = \" << 1e-9 * (st1-st0).count() << \" seconds\\n\";\n",
    "  }\n",
    "\n",
    "  {\n",
    "    std::vector<int> input(data);\n",
    "    std::cout << \"\\nRunning in parallel:\\n\";\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    std::sort(std::execution::par, input.begin(), input.end());\n",
    "    auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Parallel time = \" << 1e-9 * (pt1-pt0).count() << \" seconds\\n\";\n",
    "  }\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_sort.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_sort.sh; else ./run_sort.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fancy iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to demonstrate how the applicability of classic STL algorithms can be extended by special iterators, so called fancy iterators. Imagine the following problem: A logic sequence of pairs (key and data elements) has to be sorted according to its key element. The elements are stored in two containers, one vector that holds the keys and another vector that holds the data elements.\n",
    "\n",
    "oneDPL as part of Intel oneAPI supports fancy iterators that can be used with the C++17 parallel algorithms. For example a `counting_iterator` is provided that represents a linear increasing sequence. With one big advantage compared to a classic vector that holds the data. This can actually save memory bandwidth. Because the sequence does not need a representation in memory.\n",
    "\n",
    "In our example we use a `zip_iterator` that can be used to tie two or even multiple sequences. The resulting iterator can now be used as an input or output of STL algorithm. Like in the following example.\n",
    "\n",
    "Complete the instructions in the following code template to implement a sort by key using a zip iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/fancy_sort.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/fancy_sort.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "int main() {\n",
    "\n",
    "  {\n",
    "    using oneapi::dpl::make_zip_iterator;\n",
    "    std::vector<int> keys = { 0, 1, 0, 1, 0, 1, 0, 1, 0, 1};\n",
    "    std::vector<int> data = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
    "    // *** Step 1: create a zip iterator with iterator to begin of keys and data as input.\n",
    "    auto zip_in = data.begin();\n",
    "    const size_t n = std::distance(keys.begin(), keys.end());  \n",
    "\n",
    "    std::cout << \"\\nRunning in parallel:\\n\";\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    // *** Step 2: Replace left and right side of comparison with get<>() to extract and compare key\n",
    "    auto custom_func = [](auto l, auto r){ using std::get; return l < r;};\n",
    "    // *** Step 3: Replace all instances of iterator data.begin() with your zip iterator\n",
    "    std::sort(std::execution::par, zip_in, zip_in + n, custom_func);\n",
    "    auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Parallel time = \" << 1e-9 * (pt1-pt0).count() << \" seconds\\n\";\n",
    "      \n",
    "    std::cout << \"Sorted keys:\" ;\n",
    "    for(auto i:keys)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\" << \"Sorted data:\";\n",
    "    for(auto i:data)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/fancy_sort.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/fancy_sort.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "\n",
    "int main() {\n",
    "\n",
    "  {\n",
    "    using oneapi::dpl::make_zip_iterator;\n",
    "    std::vector<int> keys = { 0, 1, 0, 1, 0, 1, 0, 1, 0, 1};\n",
    "    std::vector<int> data = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
    "    auto zip_in = make_zip_iterator(keys.begin(), data.begin());\n",
    "    const size_t n = std::distance(keys.begin(), keys.end());  \n",
    "    auto custom_func = [](auto l, auto r){ using std::get; return get<0>(l) < get<0>(r);};\n",
    "      \n",
    "    std::cout << \"\\nRunning in parallel:\\n\";\n",
    "    auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "    std::sort(std::execution::par, zip_in, zip_in + n, custom_func);\n",
    "    auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"Parallel time = \" << 1e-9 * (pt1-pt0).count() << \" seconds\\n\";\n",
    "      \n",
    "    std::cout << \"Sorted keys:\" ;\n",
    "    for(auto i:keys)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\" << \"Sorted data:\";\n",
    "    for(auto i:data)\n",
    "      std::cout << i << \" \";\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_fancy_sort.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_fancy_sort.sh; else ./run_fancy_sort.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll demonstrate the use of an another algorithm: `transform_reduce`. It is available since C++17 and can be used to parallelize the computation of an `inner_product`.\n",
    "\n",
    "https://en.cppreference.com/w/cpp/algorithm/transform_reduce\n",
    "\n",
    "Exploring parallel execution policies which have been introduced by C++17 we shouldn't forget to mention `par_unseq`. This policy is the least restrictive by combining `par` and `unseq` policies. `unseq` policy allows the interleaving of iterations on a single thread, which is useful for vectorization. As a standalone policy, `unseq` is available since C++20. And can be useful especially for nested algorithm. The combined policy enables multi-threading and vectorization for a specific algorithm.\n",
    "\n",
    "Modify the following example by replacing the sequential policies according to the instructions in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/dot_product.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "#include <oneapi/dpl/numeric>\n",
    "\n",
    "auto exec = std::execution::seq;\n",
    "\n",
    "int main() {\n",
    "  const size_t N = 100000;\n",
    "  const size_t runs = 10;\n",
    "  double r2, r1, r;\n",
    "\n",
    "  // initialize input vectors\n",
    "  std::vector<double> a(N);\n",
    "  std::vector<double> b(N);\n",
    "\n",
    "  auto input = oneapi::dpl::counting_iterator<size_t>(0);\n",
    "  std::for_each_n(std::execution::par_unseq, input, N, [&](size_t i){ a[i] = 1.0; b[i] = 2.0; });\n",
    " \n",
    "  // First run inner product\n",
    "  auto st0 = std::chrono::high_resolution_clock::now();\n",
    "  for(int k = 0; k <= runs; ++k) {\n",
    "    if(k==1) st0 = std::chrono::high_resolution_clock::now();\n",
    "    r1 = std::inner_product(a.begin(), a.end(), b.begin(), 0.0);\n",
    "  }\n",
    "  auto st1 = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "  // Second run parallel transform reduce\n",
    "  auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "  for(int k = 0; k <= runs; ++k) {\n",
    "    if(k==1) pt0 = std::chrono::high_resolution_clock::now();\n",
    "    // *** Step 1: Replace the first parameter by execution policy par.\n",
    "    r = std::transform_reduce(exec,\n",
    "                              a.begin(), a.end(),\n",
    "                              b.begin(), 0.0);\n",
    "  }\n",
    "  auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "  \n",
    "  // Third run parallel and unsequenced transform reduce\n",
    "  auto pt2 = std::chrono::high_resolution_clock::now();\n",
    "  for(int k = 0; k <= runs; ++k) {\n",
    "    if(k==1) pt2 = std::chrono::high_resolution_clock::now();\n",
    "    // *** Step 2: Replace the first parameter by execution policy par_unseq.\n",
    "    r2 = std::transform_reduce(exec,\n",
    "                              a.begin(), a.end(),\n",
    "                              b.begin(), 0.0);\n",
    "  }\n",
    "  auto pt3 = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "  std::cout << \"Serial time    = \" << 1e-6 * (st1-st0).count() / runs << \" milliseconds\\n\";\n",
    "  std::cout << \"Parallel time  = \" << 1e-6 * (pt1-pt0).count() / runs << \" milliseconds\\n\"; \n",
    "  std::cout << \"Par_unseq time = \" << 1e-6 * (pt3-pt2).count() / runs << \" milliseconds\\n\";\n",
    "    \n",
    "  std::cout << \"Result: \" << r << \", \" << r1 << \", \" << r2 << \"\\n\";\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lab/dot_product.cpp\n",
    "//==============================================================\n",
    "// Copyright (c) 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: Apache-2.0\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "#include <oneapi/dpl/iterator>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <oneapi/dpl/algorithm>\n",
    "#include <oneapi/dpl/numeric>\n",
    "\n",
    "auto exec = std::execution::seq;\n",
    "\n",
    "int main() {\n",
    "  const size_t N = 100000;\n",
    "  const size_t runs = 10;\n",
    "  double r2, r1, r;\n",
    "\n",
    "  // initialize input vectors\n",
    "  std::vector<double> a(N);\n",
    "  std::vector<double> b(N);\n",
    "\n",
    "  auto input = oneapi::dpl::counting_iterator<size_t>(0);\n",
    "  std::for_each_n(std::execution::par_unseq, input, N, [&](size_t i){ a[i] = 1.0; b[i] = 2.0; });\n",
    " \n",
    "  // First run inner product\n",
    "  auto st0 = std::chrono::high_resolution_clock::now();\n",
    "  for(int k = 0; k <= runs; ++k) {\n",
    "    if(k==1) st0 = std::chrono::high_resolution_clock::now();\n",
    "    r1 = std::inner_product(a.begin(), a.end(), b.begin(), 0.0);\n",
    "  }\n",
    "  auto st1 = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "  // Second run parallel transform reduce\n",
    "  auto pt0 = std::chrono::high_resolution_clock::now();\n",
    "  for(int k = 0; k <= runs; ++k) {\n",
    "    if(k==1) pt0 = std::chrono::high_resolution_clock::now();\n",
    "    // *** Step 1: Replace the first parameter by execution policy par.\n",
    "    r = std::transform_reduce(std::execution::par,\n",
    "                              a.begin(), a.end(),\n",
    "                              b.begin(), 0.0);\n",
    "  }\n",
    "  auto pt1 = std::chrono::high_resolution_clock::now();\n",
    "  \n",
    "  // Third run parallel and unsequenced transform reduce\n",
    "  auto pt2 = std::chrono::high_resolution_clock::now();\n",
    "  for(int k = 0; k <= runs; ++k) {\n",
    "    if(k==1) pt2 = std::chrono::high_resolution_clock::now();\n",
    "    // *** Step 2: Replace the first parameter by execution policy par_unseq.\n",
    "    r2 = std::transform_reduce(std::execution::par_unseq,\n",
    "                              a.begin(), a.end(),\n",
    "                              b.begin(), 0.0);\n",
    "  }\n",
    "  auto pt3 = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "  std::cout << \"Serial time    = \" << 1e-6 * (st1-st0).count() / runs << \" milliseconds\\n\";\n",
    "  std::cout << \"Parallel time  = \" << 1e-6 * (pt1-pt0).count() / runs << \" milliseconds\\n\"; \n",
    "  std::cout << \"Par_unseq time = \" << 1e-6 * (pt3-pt2).count() / runs << \" milliseconds\\n\";\n",
    "    \n",
    "  std::cout << \"Result: \" << r << \", \" << r1 << \", \" << r2 << \"\\n\";\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_dot_product.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_dot_product.sh; else ./run_dot_product.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, turn off the auto vectorization and re-run. The run script has been already modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_dot_product_no_vec.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_dot_product_no_vec.sh; else ./run_dot_product_no_vec.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
